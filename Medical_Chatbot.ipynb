{"cells":[{"cell_type":"code","source":["!cp -r /content/DocProduct /content/drive/MyDrive/Projects"],"metadata":{"id":"HYw2TgZ-cS92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kAWf-BuniyeZ","executionInfo":{"status":"ok","timestamp":1728477487321,"user_tz":-330,"elapsed":109971,"user":{"displayName":"Sayan Sen","userId":"02476152609370724019"}},"outputId":"1d44bcd4-8a9c-4c1d-e68d-d70e6aadb5ba"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"RBPCBc6G60j0"},"source":["# Installing Required Packages"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Projects/DocProduct"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9PussES-JMVC","executionInfo":{"status":"ok","timestamp":1728477488551,"user_tz":-330,"elapsed":1239,"user":{"displayName":"Sayan Sen","userId":"02476152609370724019"}},"outputId":"0fcde968-1f6f-44e0-9a6d-f7990b9f35b8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Projects/DocProduct\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9878,"status":"ok","timestamp":1728477567506,"user":{"displayName":"Sayan Sen","userId":"02476152609370724019"},"user_tz":-330},"id":"7jaIDHCo6QKB","outputId":"ceb7cdaf-b269-4443-9dcb-2e6ea316608b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (4.44.2)\n","Collecting torchbearer (from -r requirements.txt (line 2))\n","  Downloading torchbearer-0.5.5-py3-none-any.whl.metadata (29 kB)\n","Collecting sacremoses (from -r requirements.txt (line 3))\n","  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n","Collecting faiss-gpu (from -r requirements.txt (line 4))\n","  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (4.66.5)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from torchbearer->-r requirements.txt (line 2)) (2.4.1+cu121)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->-r requirements.txt (line 3)) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->-r requirements.txt (line 3)) (1.4.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers->-r requirements.txt (line 1)) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers->-r requirements.txt (line 1)) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchbearer->-r requirements.txt (line 2)) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchbearer->-r requirements.txt (line 2)) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchbearer->-r requirements.txt (line 2)) (3.1.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->torchbearer->-r requirements.txt (line 2)) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->torchbearer->-r requirements.txt (line 2)) (1.3.0)\n","Downloading torchbearer-0.5.5-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-gpu, sacremoses, torchbearer\n","Successfully installed faiss-gpu-1.7.2 sacremoses-0.1.1 torchbearer-0.5.5\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"HiktryAV7AVL"},"source":["# Bert Model"]},{"cell_type":"code","source":["!python bert_train.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GTnak4iH4CB1","executionInfo":{"status":"ok","timestamp":1728052332878,"user_tz":-330,"elapsed":902577,"user":{"displayName":"Sayan Sen","userId":"02476152609370724019"}},"outputId":"05516380-6f4d-4327-aee2-3a106dee5676"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  0%|          | 0/636 [00:00<?, ?it/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Epoch [1/10], Loss: 2.105057716369629, Accuracy: 0.4937106918238994\n","Epoch [2/10], Loss: 0.8915992379188538, Accuracy: 0.589622641509434\n","Epoch [3/10], Loss: 0.8803161978721619, Accuracy: 0.6077044025157232\n","Epoch [4/10], Loss: 0.6842804551124573, Accuracy: 0.6187106918238994\n","Epoch [5/10], Loss: 1.172032356262207, Accuracy: 0.6509433962264151\n","Epoch [6/10], Loss: 0.8657556772232056, Accuracy: 0.6422955974842768\n","Epoch [7/10], Loss: 1.1043320894241333, Accuracy: 0.660377358490566\n","Epoch [8/10], Loss: 0.6491319537162781, Accuracy: 0.6619496855345912\n","Epoch [9/10], Loss: 1.343402624130249, Accuracy: 0.6816037735849056\n","Epoch [10/10], Loss: 1.613466501235962, Accuracy: 0.6894654088050315\n","Model saved successfully...\n"]}]},{"cell_type":"code","source":["!python Generate_Embeddings.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kHMJOv8VUlyv","executionInfo":{"status":"ok","timestamp":1728137085763,"user_tz":-330,"elapsed":57494,"user":{"displayName":"Sayan Sen","userId":"02476152609370724019"}},"outputId":"0f228baf-49b1-4cd8-ce35-43441780947d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Projects/DocProduct/Generate_Embeddings.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  extractor=torch.load(\"/content/drive/MyDrive/Projects/DocProduct/Trained_models/Feature_Extractor.pth\")\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["!python GPT2/GPT_data_preprocessing.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NdUK73YhRnI4","executionInfo":{"status":"ok","timestamp":1728395440490,"user_tz":-330,"elapsed":104955,"user":{"displayName":"Sayan Sen","userId":"02476152609370724019"}},"outputId":"191c5681-4a5b-4712-c030-cbe1982ab72d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Projects/DocProduct/GPT2/GPT_data_preprocessing.py:97: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  data= torch.load(\"Data/QA_with_embeddings.pt\", map_location=torch.device('cpu'))\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","FAISS Index created successfully.....\n","Token indices sequence length is longer than the specified maximum sequence length for this model (1117 > 1024). Running this sequence through the model will result in indexing errors\n","  0%|          | 0/636 [00:00<?, ?it/s]\n","length of data before filtering:  636\n","                                            question  ... gpt_lens\n","0                          how do i stop smoking now  ...     1024\n","1  i had a tubaligation 4 years ago and also have...  ...     1024\n","2  could extra caffeine consumption be a cause of...  ...     1024\n","3  hello- i am a 24 year old female 5\"4 & 115 lb ...  ...     1024\n","4  i was wanting to know if you could tell me if ...  ...     1024\n","\n","[5 rows x 7 columns]\n","length of data after filtering: 636\n","Data created and saved successfully....\n"]}]},{"cell_type":"markdown","metadata":{"id":"RG2-vUYt6K6n"},"source":["# GPT Model"]},{"cell_type":"code","source":["!python GPT_train.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W6km65N4Tr65","executionInfo":{"status":"ok","timestamp":1728398969059,"user_tz":-330,"elapsed":1976248,"user":{"displayName":"Sayan Sen","userId":"02476152609370724019"}},"outputId":"f54d3116-122e-49e4-bc1b-e2c5420a77a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 138kB/s]\n","vocab.json: 100% 1.04M/1.04M [00:00<00:00, 5.30MB/s]\n","merges.txt: 100% 456k/456k [00:00<00:00, 2.44MB/s]\n","tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 4.35MB/s]\n","config.json: 100% 665/665 [00:00<00:00, 3.65MB/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","model.safetensors: 100% 548M/548M [00:04<00:00, 135MB/s]\n","generation_config.json: 100% 124/124 [00:00<00:00, 596kB/s]\n","Epoch 1:   0%|          | 0/318 [00:00<?, ?it/s]\n","Epoch 1 finished with average loss: 5.444678802910091\n","Epoch 2:   0%|          | 0/318 [00:00<?, ?it/s]\n","Epoch 2 finished with average loss: 4.57387343817537\n","Epoch 3:   0%|          | 0/318 [00:00<?, ?it/s]\n","Epoch 3 finished with average loss: 4.088991416325359\n","Epoch 4:   0%|          | 0/318 [00:00<?, ?it/s]\n","Epoch 4 finished with average loss: 3.628928504649948\n","Epoch 5:   0%|          | 0/318 [00:00<?, ?it/s]\n","Epoch 5 finished with average loss: 3.2168391080772354\n","Epoch 6:   0%|          | 0/318 [00:00<?, ?it/s]\n","Epoch 6 finished with average loss: 2.8196661916543855\n","Epoch 7:   0%|          | 0/318 [00:00<?, ?it/s]\n","Epoch 7 finished with average loss: 2.4957558784469867\n","Epoch 8:   0%|          | 0/318 [00:00<?, ?it/s]\n","Epoch 8 finished with average loss: 2.2089585575667567\n","Epoch 9:   0%|          | 0/318 [00:00<?, ?it/s]\n","Epoch 9 finished with average loss: 1.9352105932790529\n","Epoch 10:   0%|          | 0/318 [00:00<?, ?it/s]\n","Epoch 10 finished with average loss: 1.7496253200015932\n","Model saved successfully...\n"]}]},{"cell_type":"code","source":["!python infernce.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":457},"id":"Hf9f_JAq3VqF","executionInfo":{"status":"error","timestamp":1728477980021,"user_tz":-330,"elapsed":25045,"user":{"displayName":"Sayan Sen","userId":"02476152609370724019"}},"outputId":"f50ed851-d8e8-4b72-eea9-1e1a12f91769"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-14410fae90a2>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model = torch.load(\"Trained_models/chatbot_model.pth\", map_location=torch.device('cpu'))\n"]},{"output_type":"stream","name":"stdout","text":["Chatbot is ready! Type 'exit' to quit.\n","You: Hi\n","Chatbot: - wa ) is a type of hi-h which is a family of seven. it is a region of the ( ) in ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of ( ) of (\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-14410fae90a2>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Run the chatbot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mchatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-14410fae90a2>\u001b[0m in \u001b[0;36mchatbot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Get user input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Exit the chatbot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMBcVbNIKPeahq37HXMMe+9"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}